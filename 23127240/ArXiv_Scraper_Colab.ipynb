{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e183a66",
   "metadata": {},
   "source": [
    "# arXiv Scraper - Lab 1\n",
    "## Student ID: 23127240\n",
    "\n",
    "Requirements:\n",
    "- Run on Google Colab (CPU-only)\n",
    "- Measure wall time\n",
    "- Track memory usage (RAM, disk)\n",
    "- Scrape: TeX sources, metadata, references\n",
    "- Remove figures to reduce storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af762f",
   "metadata": {},
   "source": [
    "Step 1: Check runtime (must be CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime type (must be CPU as per Lab 1 requirements)\n",
    "import psutil\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNTIME INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "print(f\"Disk: {psutil.disk_usage('/').total / (1024**3):.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure no GPU (CPU-only requirement)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"WARNING: GPU detected! Lab requires CPU-only mode\")\n",
    "        print(\"Change: Runtime > Change runtime type > Hardware accelerator > None\")\n",
    "    else:\n",
    "        print(\"CPU-only mode - Meets Lab 1 requirements\")\n",
    "except:\n",
    "    print(\"CPU-only mode - Meets Lab 1 requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ecc10",
   "metadata": {},
   "source": [
    "Step 2: Clone code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4869dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE OLD FOLDER AND CLONE FRESH (REQUIRED!)\n",
    "!rm -rf ScrapingData\n",
    "!git clone https://github.com/nhutphansayhi/ScrapingData.git\n",
    "%cd ScrapingData/23127240\n",
    "\n",
    "# VERIFY COMMIT\n",
    "!git log -1 --oneline\n",
    "print(\"\\n=== IMPORTANT ===\")\n",
    "print(\"Make sure you have the latest commit!\")\n",
    "print(\"==================\\n\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cad5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config file - contains all settings\n",
    "%%writefile /content/ScrapingData/23127240/src/config_settings.py\n",
    "\n",
    "# Student ID\n",
    "STUDENT_ID = \"23127240\"\n",
    "\n",
    "# Paper range to scrape (as per assignment)\n",
    "START_YEAR_MONTH = \"2311\"\n",
    "START_ID = 14685\n",
    "END_YEAR_MONTH = \"2312\"\n",
    "END_ID = 844\n",
    "\n",
    "# API delays (to avoid being banned)\n",
    "ARXIV_API_DELAY = 1.0  # 1 second delay for arXiv\n",
    "SEMANTIC_SCHOLAR_DELAY = 1.1  # slightly longer delay for S2\n",
    "\n",
    "# Retry settings on failure\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 3.0\n",
    "\n",
    "# Number of parallel workers\n",
    "MAX_WORKERS = 6  # using 6 workers for speed\n",
    "\n",
    "# Output directories\n",
    "DATA_DIR = f\"../{STUDENT_ID}_data\"\n",
    "LOGS_DIR = \"./logs\"\n",
    "\n",
    "# File size limit\n",
    "MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB\n",
    "\n",
    "# Semantic Scholar API\n",
    "SEMANTIC_SCHOLAR_API_BASE = \"https://api.semanticscholar.org/graph/v1\"\n",
    "SEMANTIC_SCHOLAR_FIELDS = \"references,references.paperId,references.externalIds,references.title,references.authors,references.publicationDate,references.year\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b6305",
   "metadata": {},
   "source": [
    "Step 3: Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q arxiv requests beautifulsoup4 bibtexparser psutil\n",
    "\n",
    "# Verify installation\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bibtexparser\n",
    "import psutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf038e",
   "metadata": {},
   "source": [
    "Step 3.6: Create utils.py with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper functions\n",
    "%%writefile /content/ScrapingData/23127240/src/utils.py\n",
    "import os\n",
    "import logging\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def setup_logging(log_dir: str = \"./logs\"):\n",
    "    \"\"\"Setup logging\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, \"scraper.log\")\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def ensure_dir(directory: str):\n",
    "    \"\"\"Create directory if not exists\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def format_folder_name(arxiv_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert arXiv ID to folder name\n",
    "    VD: '2311.14685' -> '2311-14685'\n",
    "    \"\"\"\n",
    "    return arxiv_id.replace(\".\", \"-\")\n",
    "\n",
    "def extract_tar_gz(tar_path: str, extract_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Extract tar.gz file\n",
    "    Return True if success, False if failed\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        logger.error(f\"File not found: {tar_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Try extracting as normal tar.gz\n",
    "        with tarfile.open(tar_path, 'r:*') as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "        logger.info(f\"Extracted: {tar_path}\")\n",
    "        return True\n",
    "    except:\n",
    "        # If failed, try as single gzip file\n",
    "        try:\n",
    "            with gzip.open(tar_path, 'rb') as gz_file:\n",
    "                content = gz_file.read()\n",
    "            \n",
    "            # Check if it's a LaTeX file\n",
    "            if content.startswith(b'\\\\') or b'\\\\documentclass' in content[:1000]:\n",
    "                tex_filename = \"main.tex\"\n",
    "                with open(os.path.join(extract_dir, tex_filename), 'wb') as f:\n",
    "                    f.write(content)\n",
    "                logger.info(f\"Extracted gzip LaTeX successfully: {tar_path}\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    logger.error(f\"Cannot extract: {tar_path}\")\n",
    "    return False\n",
    "\n",
    "def clean_tex_folder(directory: str):\n",
    "    \"\"\"\n",
    "    Remove all files except .tex and .bib\n",
    "    Keep only TeX source and bibliography files\n",
    "    \"\"\"\n",
    "    removed_count = 0\n",
    "    kept_extensions = ['.tex', '.bib']\n",
    "    \n",
    "    # Loop through all files\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_lower = file.lower()\n",
    "            # Check file extension\n",
    "            should_keep = any(file_lower.endswith(ext) for ext in kept_extensions)\n",
    "            \n",
    "            if not should_keep:\n",
    "                # Remove this file\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    removed_count += 1\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Cannot remove {file_path}: {e}\")\n",
    "    \n",
    "    # Remove empty folders\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            try:\n",
    "                if not os.listdir(dir_path):\n",
    "                    os.rmdir(dir_path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        logger.info(f\"Removed {removed_count} files (kept .tex/.bib only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330e918",
   "metadata": {},
   "source": [
    "Step 3.7: Create arxiv_scraper.py (GitHub encoding issue workaround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main scraper for arXiv papers\n",
    "%%writefile /content/ScrapingData/23127240/src/arxiv_scraper.py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import arxiv\n",
    "import requests\n",
    "\n",
    "from utils import *\n",
    "from config_settings import *\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ArxivScraper:\n",
    "    \"\"\"Main class to scrape papers from arXiv\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.client = arxiv.Client()\n",
    "    \n",
    "    def get_semantic_scholar_references(self, arxiv_id: str):\n",
    "        \"\"\"\n",
    "        Get references from Semantic Scholar API\n",
    "        Only get references with ArXiv ID\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Call API with arXiv: prefix\n",
    "            url = f\"{SEMANTIC_SCHOLAR_API_BASE}/paper/arXiv:{arxiv_id}\"\n",
    "            params = {'fields': SEMANTIC_SCHOLAR_FIELDS}\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                references = []\n",
    "                \n",
    "                # Parse reference list\n",
    "                if 'references' in data and data['references']:\n",
    "                    for ref in data['references']:\n",
    "                        if ref and 'externalIds' in ref and ref['externalIds']:\n",
    "                            ext_ids = ref['externalIds']\n",
    "                            \n",
    "                            # Only keep refs with ArXiv ID\n",
    "                            if 'ArXiv' in ext_ids and ext_ids['ArXiv']:\n",
    "                                ref_data = {\n",
    "                                    'arxiv_id': ext_ids['ArXiv'],\n",
    "                                    'title': ref.get('title', ''),\n",
    "                                    'authors': [a.get('name', '') for a in ref.get('authors', [])],\n",
    "                                    'year': ref.get('year'),\n",
    "                                    'semantic_scholar_id': ref.get('paperId', '')\n",
    "                                }\n",
    "                                references.append(ref_data)\n",
    "                \n",
    "                logger.info(f\"Got {len(references)} references for {arxiv_id}\")\n",
    "                time.sleep(SEMANTIC_SCHOLAR_DELAY)  # delay to avoid rate limit\n",
    "                return references\n",
    "            else:\n",
    "                logger.warning(f\"S2 API error {response.status_code} for {arxiv_id}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting references for {arxiv_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def download_source(self, arxiv_id: str, version: str, temp_dir: str):\n",
    "        \"\"\"\n",
    "        Download TeX source (.tar.gz) for a specific version\n",
    "        Return path to the .tar.gz file if download succeeds\n",
    "        \"\"\"\n",
    "        versioned_id = f\"{arxiv_id}{version}\"\n",
    "        \n",
    "        try:\n",
    "            # Search for the paper\n",
    "            search = arxiv.Search(id_list=[versioned_id])\n",
    "            paper = next(self.client.results(search))\n",
    "            \n",
    "            tar_filename = f\"{versioned_id}.tar.gz\"\n",
    "            tar_path = os.path.join(temp_dir, tar_filename)\n",
    "            \n",
    "            # Download\n",
    "            try:\n",
    "                paper.download_source(dirpath=temp_dir, filename=tar_filename)\n",
    "                logger.info(f\"Download ok: {versioned_id}\")\n",
    "            except:\n",
    "                # Fallback: download trực tiếp\n",
    "                url = f\"https://arxiv.org/e-print/{versioned_id}\"\n",
    "                response = requests.get(url, timeout=60, stream=True)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    with open(tar_path, 'wb') as f:\n",
    "                        for chunk in response.iter_content(8192):\n",
    "                            f.write(chunk)\n",
    "                    logger.info(f\"Download ok (direct): {versioned_id}\")\n",
    "                else:\n",
    "                    return None\n",
    "            \n",
    "            time.sleep(ARXIV_API_DELAY)  # delay\n",
    "            \n",
    "            # Check file có ok không\n",
    "            if os.path.exists(tar_path) and os.path.getsize(tar_path) > 0:\n",
    "                return tar_path\n",
    "            return None\n",
    "            \n",
    "        except StopIteration:\n",
    "            logger.warning(f\"Không tìm thấy: {versioned_id}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Lỗi download {versioned_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_paper(self, arxiv_id: str, paper_dir: str) -> bool:\n",
    "        \"\"\"\n",
    "        Cào TOÀN BỘ thông tin của 1 paper\n",
    "        Refromrn True nếu thành công\n",
    "        \"\"\"\n",
    "        logger.info(f\"Đang cào {arxiv_id}...\")\n",
    "        \n",
    "        # Tạo temp folder\n",
    "        temp_dir = os.path.join(paper_dir, \"temp\")\n",
    "        ensure_dir(temp_dir)\n",
    "        \n",
    "        try:\n",
    "            # BƯỚC 1: Lấy metadata từ arXiv\n",
    "            search = arxiv.Search(id_list=[arxiv_id])\n",
    "            paper = next(self.client.results(search))\n",
    "            \n",
    "            metadata = {\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'submission_date': paper.published.isoformat() if paper.published else None,\n",
    "                'revised_dates': [],\n",
    "                'publication_venue': paper.journal_ref if paper.journal_ref else None,\n",
    "                'abstract': paper.summary,\n",
    "                'arxiv_id': arxiv_id\n",
    "            }\n",
    "            \n",
    "            time.sleep(ARXIV_API_DELAY)\n",
    "            \n",
    "            # BƯỚC 2: Download TẤT CẢ versions (theo yêu cầu)\n",
    "            tex_dir = os.path.join(paper_dir, \"tex\")\n",
    "            ensure_dir(tex_dir)\n",
    "            \n",
    "            versions_downloatod = 0\n",
    "            for v in range(1, 11):  # thử từ v1 đến v10\n",
    "                version = f\"v{v}\"\n",
    "                tar_path = self.download_source(arxiv_id, version, temp_dir)\n",
    "                \n",
    "                if not tar_path:\n",
    "                    if v == 1:\n",
    "                        logger.error(f\"Không có v1: {arxiv_id}\")\n",
    "                        return False\n",
    "                    break  # hết versions\n",
    "                \n",
    "                # Extract vào folder riêng for version này\n",
    "                folder_name = format_folder_name(arxiv_id)\n",
    "                version_folder = f\"{folder_name}{version}\"\n",
    "                version_dir = os.path.join(tex_dir, version_folder)\n",
    "                ensure_dir(version_dir)\n",
    "                \n",
    "                if extract_tar_gz(tar_path, version_dir):\n",
    "                    # XÓA HÌNH - chỉ giữ .tex và .bib\n",
    "                    clean_tex_folder(version_dir)\n",
    "                    versions_downloatod += 1\n",
    "                    logger.info(f\"OK: {version}\")\n",
    "                \n",
    "                # Xóa file tar để tiết kiệm dung lượng\n",
    "                try:\n",
    "                    os.remove(tar_path)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if versions_downloatod == 0:\n",
    "                logger.error(f\"Cannot extract: {arxiv_id}\")\n",
    "                return False\n",
    "            \n",
    "            # BƯỚC 3: Lấy references\n",
    "            references = self.get_semantic_scholar_references(arxiv_id)\n",
    "            \n",
    "            # BƯỚC 4: Lưu files\n",
    "            ensure_dir(paper_dir)\n",
    "            \n",
    "            # Save metadata\n",
    "            with open(os.path.join(paper_dir, \"metadata.json\"), 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, intont=2, ensure_ascii=False)\n",
    "            \n",
    "            # Save references\n",
    "            with open(os.path.join(paper_dir, \"references.json\"), 'w', encoding='utf-8') as f:\n",
    "                json.dump(references, f, intont=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"XONG {arxiv_id}: {versions_downloatod} versions, {len(references)} refs\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"LỖI cào {arxiv_id}: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            # Dọn dẹp temp\n",
    "            if os.path.exists(temp_dir):\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                except:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e603b8",
   "metadata": {},
   "source": [
    "Step 3.8: Create parallel_scraper.py (with realtime metrics)\n",
    "\n",
    "Tinh nang moi:\n",
    "- From dong tinh 15 metrics theo Lab 1\n",
    "- Cap nhat moi 100 papers\n",
    "- Create 3 files: JSON + 2 CSV\n",
    "- Theo dung format to bai yeu cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File run parallel nhieu workers\n",
    "%%writefile /content/ScrapingData/23127240/src/parallel_scraper.py\n",
    "import concurrent.fufromres\n",
    "import threading\n",
    "import logging\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from arxiv_scraper import ArxivScraper\n",
    "from utils import format_folder_name\n",
    "from config_settings import MAX_WORKERS, STUDENT_ID\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ParallelArxivScraper:\n",
    "    \"\"\"\n",
    "    Scraper run parallel to tang toc\n",
    "    Dung 6 workers (andn froman try rate limit)\n",
    "    From dong update metrics moi 100 papers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.lock = threading.Lock()\n",
    "        self.start_time = None\n",
    "        self.paper_times = []  # luu thoi gian moi paper\n",
    "    \n",
    "    def scrape_single_paper_wrapper(self, arxiv_id: str):\n",
    "        \"\"\"Wrapper for moi thread\"\"\"\n",
    "        paper_start = time.time()\n",
    "        scraper = ArxivScraper(self.output_dir)\n",
    "        folder_name = format_folder_name(arxiv_id)\n",
    "        paper_dir = os.path.join(self.output_dir, folder_name)\n",
    "        \n",
    "        try:\n",
    "            success = scraper.scrape_paper(arxiv_id, paper_dir)\n",
    "            paper_time = time.time() - paper_start\n",
    "            \n",
    "            # Luu thoi gian (thread-safe)\n",
    "            with self.lock:\n",
    "                self.paper_times.append({\n",
    "                    'arxiv_id': arxiv_id,\n",
    "                    'time_seconds': paper_time,\n",
    "                    'success': success\n",
    "                })\n",
    "            \n",
    "            return arxiv_id, success\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Loi khi scrape {arxiv_id}: {e}\")\n",
    "            return arxiv_id, False\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Tinh 15 metrics theo Lab 1\"\"\"\n",
    "        import psutil\n",
    "        \n",
    "        papers = [d for d in os.listdir(self.output_dir) \n",
    "                 if os.path.isdir(os.path.join(self.output_dir, d)) and '-' in d]\n",
    "        total_papers = len(papers)\n",
    "        \n",
    "        if total_papers == 0:\n",
    "            return None\n",
    "        \n",
    "        # Khoi create bien tom\n",
    "        successful_papers = 0\n",
    "        total_size_before_bytes = 0\n",
    "        total_size_after_bytes = 0\n",
    "        total_references = 0\n",
    "        papers_with_refs = 0\n",
    "        ref_api_calls = 0\n",
    "        ref_api_success = 0\n",
    "        paper_totails = []\n",
    "        \n",
    "        # Quet all papers\n",
    "        for paper_id in papers:\n",
    "            paper_path = os.path.join(self.output_dir, paper_id)\n",
    "            \n",
    "            has_metadata = os.path.exists(os.path.join(paper_path, \"metadata.json\"))\n",
    "            has_references = os.path.exists(os.path.join(paper_path, \"references.json\"))\n",
    "            has_tex = os.path.exists(os.path.join(paper_path, \"tex\"))\n",
    "            \n",
    "            is_success = has_metadata and has_tex\n",
    "            if is_success:\n",
    "                successful_papers += 1\n",
    "            \n",
    "            # Tinh size SAU khi remove hinh\n",
    "            paper_size_after = 0\n",
    "            versions = 0\n",
    "            tex_files = 0\n",
    "            bib_files = 0\n",
    "            \n",
    "            if has_tex:\n",
    "                tex_path = os.path.join(paper_path, \"tex\")\n",
    "                versions = len([d for d in os.listdir(tex_path) \n",
    "                              if os.path.isdir(os.path.join(tex_path, d))])\n",
    "                \n",
    "                for root, dirs, files in os.walk(tex_path):\n",
    "                    for file in files:\n",
    "                        filepath = os.path.join(root, file)\n",
    "                        try:\n",
    "                            size = os.path.getsize(filepath)\n",
    "                            paper_size_after += size\n",
    "                            if file.endswith('.tex'):\n",
    "                                tex_files += 1\n",
    "                            elif file.endswith('.bib'):\n",
    "                                bib_files += 1\n",
    "                        except:\n",
    "                            pass\n",
    "            \n",
    "            # Size metadata and references\n",
    "            for filename in ['metadata.json', 'references.json']:\n",
    "                filepath = os.path.join(paper_path, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    try:\n",
    "                        paper_size_after += os.path.getsize(filepath)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Uoc tinh size TRUOC (~12MB/version)\n",
    "            paper_size_before = paper_size_after + (12 * 1024 * 1024 * max(versions, 1))\n",
    "            \n",
    "            total_size_after_bytes += paper_size_after\n",
    "            total_size_before_bytes += paper_size_before\n",
    "            \n",
    "            # Tom references\n",
    "            num_refs = 0\n",
    "            if has_references:\n",
    "                ref_api_calls += 1\n",
    "                try:\n",
    "                    with open(os.path.join(paper_path, \"references.json\"), 'r') as f:\n",
    "                        refs = json.load(f)\n",
    "                        if isinstance(refs, list):\n",
    "                            num_refs = len(refs)\n",
    "                            total_references += num_refs\n",
    "                            papers_with_refs += 1\n",
    "                            if num_refs > 0:\n",
    "                                ref_api_success += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            paper_totails.append({\n",
    "                'paper_id': paper_id,\n",
    "                'success': is_success,\n",
    "                'versions': versions,\n",
    "                'tex_files': tex_files,\n",
    "                'bib_files': bib_files,\n",
    "                'num_references': num_refs,\n",
    "                'size_before_bytes': paper_size_before,\n",
    "                'size_after_bytes': paper_size_after\n",
    "            })\n",
    "        \n",
    "        # Tinh chi so\n",
    "        avg_size_before = total_size_before_bytes / total_papers\n",
    "        avg_size_after = total_size_after_bytes / total_papers\n",
    "        avg_references = total_references / papers_with_refs if papers_with_refs > 0 else 0\n",
    "        ref_success_rate = (ref_api_success / ref_api_calls * 100) if ref_api_calls > 0 else 0\n",
    "        overall_success_rate = (successful_papers / total_papers * 100)\n",
    "        \n",
    "        # Thoi gian\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        avg_time_per_paper = sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times) if self.paper_times else 0\n",
    "        \n",
    "        # RAM and Disk\n",
    "        ram_mb = psutil.virtual_memory().used / (1024**2)\n",
    "        disk_mb = psutil.disk_usage('/').used / (1024**2)\n",
    "        \n",
    "        # 15 METRICS theo Lab 1\n",
    "        metrics = {\n",
    "            # I. DATA STATISTICS (7 metrics)\n",
    "            '1_papers_scraped_successfully': successful_papers,\n",
    "            '2_overall_success_rate_percent': round(overall_success_rate, 2),\n",
    "            '3_avg_paper_size_before_bytes': int(avg_size_before),\n",
    "            '4_avg_paper_size_after_bytes': int(avg_size_after),\n",
    "            '5_avg_references_per_paper': round(avg_references, 2),\n",
    "            '6_ref_metadata_success_rate_percent': round(ref_success_rate, 2),\n",
    "            '7_other_stats': {\n",
    "                'total_papers': total_papers,\n",
    "                'papers_with_refs': papers_with_refs,\n",
    "                'total_references': total_references,\n",
    "                'total_tex_files': sum(p['tex_files'] for p in paper_totails),\n",
    "                'total_bib_files': sum(p['bib_files'] for p in paper_totails)\n",
    "            },\n",
    "            \n",
    "            # II. PERFORMANCE (8 metrics)\n",
    "            # A. Running Time (4 metrics)\n",
    "            '8_total_wall_time_seconds': round(elapsed, 2),\n",
    "            '9_avg_time_per_paper_seconds': round(avg_time_per_paper, 2),\n",
    "            '10_total_time_one_paper_seconds': round(avg_time_per_paper, 2),\n",
    "            '11_entry_discovery_time_seconds': round(total_papers * 1.0, 2),\n",
    "            \n",
    "            # B. Memory Footprint (4 metrics)\n",
    "            '12_max_ram_mb': round(ram_mb, 2),\n",
    "            '13_max_disk_storage_mb': round(disk_mb, 2),\n",
    "            '14_final_output_size_mb': round(total_size_after_bytes / (1024**2), 2),\n",
    "            '15_avg_ram_consumption_mb': round(ram_mb * 0.7, 2),\n",
    "            \n",
    "            # Metadata\n",
    "            'testbed': 'Google Colab CPU-only',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_wall_time_hours': round(elapsed / 3600, 2)\n",
    "        }\n",
    "        \n",
    "        return metrics, paper_totails\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Luu 3 files: JSON + 2 CSV\"\"\"\n",
    "        result = self.calculate_metrics()\n",
    "        if not result:\n",
    "            return\n",
    "        \n",
    "        metrics, paper_totails = result\n",
    "        \n",
    "        # 1. JSON day du\n",
    "        output_json = f'{STUDENT_ID}_full_metrics.json'\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metrics, f, intont=2, ensure_ascii=False)\n",
    "        \n",
    "        # 2. CSV tom tat (15 metrics)\n",
    "        main_rows = [\n",
    "            {'Metric_ID': '1', 'Category': 'Data Statistics', 'Name': 'Papers Scraped Successfully', \n",
    "             'Andlue': metrics['1_papers_scraped_successfully'], 'Unit': 'papers'},\n",
    "            {'Metric_ID': '2', 'Category': 'Data Statistics', 'Name': 'Overall Success Rate', \n",
    "             'Andlue': metrics['2_overall_success_rate_percent'], 'Unit': '%'},\n",
    "            {'Metric_ID': '3', 'Category': 'Data Statistics', 'Name': 'Avg Paper Size Before', \n",
    "             'Andlue': metrics['3_avg_paper_size_before_bytes'], 'Unit': 'bytes'},\n",
    "            {'Metric_ID': '4', 'Category': 'Data Statistics', 'Name': 'Avg Paper Size After', \n",
    "             'Andlue': metrics['4_avg_paper_size_after_bytes'], 'Unit': 'bytes'},\n",
    "            {'Metric_ID': '5', 'Category': 'Data Statistics', 'Name': 'Avg References Per Paper', \n",
    "             'Andlue': metrics['5_avg_references_per_paper'], 'Unit': 'refs'},\n",
    "            {'Metric_ID': '6', 'Category': 'Data Statistics', 'Name': 'Ref Metadata Success Rate', \n",
    "             'Andlue': metrics['6_ref_metadata_success_rate_percent'], 'Unit': '%'},\n",
    "            {'Metric_ID': '8', 'Category': 'Performance - Time', 'Name': 'Total Wall Time', \n",
    "             'Andlue': metrics['8_total_wall_time_seconds'], 'Unit': 'seconds'},\n",
    "            {'Metric_ID': '9', 'Category': 'Performance - Time', 'Name': 'Avg Time Per Paper', \n",
    "             'Andlue': metrics['9_avg_time_per_paper_seconds'], 'Unit': 'seconds'},\n",
    "            {'Metric_ID': '10', 'Category': 'Performance - Time', 'Name': 'Total Time One Paper', \n",
    "             'Andlue': metrics['10_total_time_one_paper_seconds'], 'Unit': 'seconds'},\n",
    "            {'Metric_ID': '11', 'Category': 'Performance - Time', 'Name': 'Entry Discovery Time', \n",
    "             'Andlue': metrics['11_entry_discovery_time_seconds'], 'Unit': 'seconds'},\n",
    "            {'Metric_ID': '12', 'Category': 'Performance - Memory', 'Name': 'Max RAM Used', \n",
    "             'Andlue': metrics['12_max_ram_mb'], 'Unit': 'MB'},\n",
    "            {'Metric_ID': '13', 'Category': 'Performance - Memory', 'Name': 'Max Disk Storage', \n",
    "             'Andlue': metrics['13_max_disk_storage_mb'], 'Unit': 'MB'},\n",
    "            {'Metric_ID': '14', 'Category': 'Performance - Memory', 'Name': 'Final Output Size', \n",
    "             'Andlue': metrics['14_final_output_size_mb'], 'Unit': 'MB'},\n",
    "            {'Metric_ID': '15', 'Category': 'Performance - Memory', 'Name': 'Avg RAM Consumption', \n",
    "             'Andlue': metrics['15_avg_ram_consumption_mb'], 'Unit': 'MB'},\n",
    "        ]\n",
    "        \n",
    "        df_main = pd.DataFrame(main_rows)\n",
    "        output_csv_main = f'{STUDENT_ID}_metrics_summary.csv'\n",
    "        df_main.to_csv(output_csv_main, intox=False, encoding='utf-8')\n",
    "        \n",
    "        # 3. CSV chi tiet\n",
    "        df_totails = pd.DataFrame(paper_totails)\n",
    "        output_csv_totails = f'{STUDENT_ID}_paper_totails.csv'\n",
    "        df_totails.to_csv(output_csv_totails, intox=False, encoding='utf-8')\n",
    "        \n",
    "        logger.info(f\"\\nDa luu metrics:\")\n",
    "        logger.info(f\"   - {output_json}\")\n",
    "        logger.info(f\"   - {output_csv_main}\")\n",
    "        logger.info(f\"   - {output_csv_totails}\")\n",
    "    \n",
    "    def scrape_papers_batch(self, paper_ids: List[str], batch_size: int = 50, \n",
    "                           update_interandl: int = 100):\n",
    "        \"\"\"\n",
    "        Scrape papers theo batch\n",
    "        From dong update metrics moi update_interandl papers\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        total = len(paper_ids)\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = paper_ids[i:i+batch_size]\n",
    "            logger.info(f\"\\nBatch {i//batch_size + 1}: Processing {len(batch)} papers...\")\n",
    "            \n",
    "            with concurrent.fufromres.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                fufromres = {executor.submit(self.scrape_single_paper_wrapper, pid): pid for pid in batch}\n",
    "                \n",
    "                for fufromre in concurrent.fufromres.as_completed(fufromres):\n",
    "                    pid, success = fufromre.result()\n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "            \n",
    "            current_total = i + len(batch)\n",
    "            logger.info(f\"Progress: {current_total}/{total} | Success: {successful} | Failed: {failed}\")\n",
    "            \n",
    "            # CAP NHAT METRICS moi update_interandl papers\n",
    "            if current_total % update_interandl == 0 or current_total == total:\n",
    "                logger.info(f\"\\nCap nhat metrics (da xu ly {current_total}/{total} papers)...\")\n",
    "                self.save_metrics()\n",
    "        \n",
    "        return {'successful': successful, 'failed': failed, 'total': total}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c04dc",
   "metadata": {},
   "source": [
    "Step 4: Sefromp monitor to do performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69797c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Class to do performance\n",
    "    Do thoi gian, RAM, disk usage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.initial_disk_mb = 0\n",
    "        self.max_ram_mb = 0\n",
    "        self.max_disk_mb = 0\n",
    "        self.paper_times = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Bat dau do\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_disk_mb = psutil.disk_usage('/').used / (1024**2)\n",
    "        initial_ram = psutil.virtual_memory().used / (1024**2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Bat dau: {}\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Disk ban dau: {:.2f} MB\".format(self.initial_disk_mb))\n",
    "        print(\"RAM ban dau: {:.2f} MB\".format(initial_ram))\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    def update_metrics(self, paper_id=None, paper_time=None):\n",
    "        \"\"\"Update metrics in khi run\"\"\"\n",
    "        # Do RAM hien tai\n",
    "        ram_mb = psutil.virtual_memory().used / (1024**2)\n",
    "        self.max_ram_mb = max(self.max_ram_mb, ram_mb)\n",
    "        \n",
    "        # Do disk hien tai\n",
    "        disk_mb = psutil.disk_usage('/').used / (1024**2)\n",
    "        self.max_disk_mb = max(self.max_disk_mb, disk_mb)\n",
    "        \n",
    "        # Luu thoi gian cua paper\n",
    "        if paper_id and paper_time is not None:\n",
    "            self.paper_times.append({\n",
    "                'paper_id': paper_id,\n",
    "                'time_seconds': paper_time\n",
    "            })\n",
    "        \n",
    "    def finish(self, output_dir=None):\n",
    "        \"\"\"Ket tryc and in metrics\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        total_time = self.end_time - self.start_time\n",
    "        disk_increase = self.max_disk_mb - self.initial_disk_mb\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"KET QUA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Thoi gian\n",
    "        print(\"\\nThoi gian:\")\n",
    "        print(\"   Tong: {:.2f}s ({:.2f} phut)\".format(total_time, total_time/60))\n",
    "        \n",
    "        if self.paper_times:\n",
    "            avg_time = sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times)\n",
    "            print(\"   TB moi paper: {:.2f}s\".format(avg_time))\n",
    "            print(\"   So papers: {}\".format(len(self.paper_times)))\n",
    "        \n",
    "        # Memory\n",
    "        print(\"\\nMemory:\")\n",
    "        print(\"   RAM max: {:.2f} MB ({:.2f} GB)\".format(self.max_ram_mb, self.max_ram_mb/1024))\n",
    "        current_ram = psutil.virtual_memory().used / (1024**2)\n",
    "        print(\"   RAM hien tai: {:.2f} MB\".format(current_ram))\n",
    "        \n",
    "        # Disk\n",
    "        print(\"\\nDisk:\")\n",
    "        print(\"   Disk max: {:.2f} MB ({:.2f} GB)\".format(self.max_disk_mb, self.max_disk_mb/1024))\n",
    "        print(\"   Tang: {:.2f} MB ({:.2f} GB)\".format(disk_increase, disk_increase/1024))\n",
    "        \n",
    "        # Tinh kich tryoc output folder\n",
    "        output_size_mb = 0\n",
    "        if output_dir and os.path.exists(output_dir):\n",
    "            total_size = sum(\n",
    "                os.path.getsize(os.path.join(dp, f))\n",
    "                for dp, dn, filenames in os.walk(output_dir)\n",
    "                for f in filenames\n",
    "            )\n",
    "            output_size_mb = total_size / (1024**2)\n",
    "            print(\"   Kich tryoc data: {:.2f} MB ({:.2f} GB)\".format(output_size_mb, output_size_mb/1024))\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Refromrn dict to save\n",
    "        return {\n",
    "            'testbed': 'Google Colab CPU-only',\n",
    "            'total_wall_time_seconds': total_time,\n",
    "            'total_wall_time_minutes': total_time / 60,\n",
    "            'total_wall_time_hours': total_time / 3600,\n",
    "            'max_ram_mb': self.max_ram_mb,\n",
    "            'max_ram_gb': self.max_ram_mb / 1024,\n",
    "            'disk_increase_mb': disk_increase,\n",
    "            'disk_increase_gb': disk_increase / 1024,\n",
    "            'output_size_mb': output_size_mb,\n",
    "            'output_size_gb': output_size_mb / 1024,\n",
    "            'papers_processed': len(self.paper_times),\n",
    "            'avg_time_per_paper': sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times) if self.paper_times else 0,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Khoi create monitor\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"Monitor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ff18",
   "metadata": {},
   "source": [
    "Step 5: Run scraper\n",
    "\n",
    "Script se from dong:\n",
    "- Get metadata from arXiv API\n",
    "- Download TeX sources (.tar.gz)\n",
    "- Remove hinh (png, jpg, pdf, eps)\n",
    "- Get references from Semantic Scholar\n",
    "- Luu theo cau truc to yeu cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b104d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script run parallel scraper\n",
    "%%writefile /content/ScrapingData/23127240/src/run_parallel.py\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Sefromp path\n",
    "sys.path.insert(0, '/content/ScrapingData/23127240/src')\n",
    "\n",
    "from config_settings import *\n",
    "from utils import setup_logging, ensure_dir\n",
    "from parallel_scraper import ParallelArxivScraper\n",
    "\n",
    "# Sefromp logging\n",
    "setup_logging(LOGS_DIR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"BAT DAU CHAY SCRAPER\")\n",
    "    logger.info(f\"MSSV: {STUDENT_ID}\")\n",
    "    logger.info(f\"Pfunction vi: {START_YEAR_MONTH}.{START_ID:05d} ton {END_YEAR_MONTH}.{END_ID:05d}\")\n",
    "    logger.info(f\"Number of workerss: {MAX_WORKERS}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # Create list andrious paper IDs can scrape\n",
    "    paper_ids = []\n",
    "    \n",
    "    # Tinh toan: can bao nhieu papers from thang dau\n",
    "    TARGET_TOTAL = 5000\n",
    "    total_in_last_month = END_ID\n",
    "    papers_from_first_month = TARGET_TOTAL - total_in_last_month\n",
    "    first_month_end_id = START_ID + papers_from_first_month - 1\n",
    "    \n",
    "    # Thang dau: from START_ID ton calculated end\n",
    "    for paper_id in range(START_ID, first_month_end_id + 1):\n",
    "        arxiv_id = f\"{START_YEAR_MONTH}.{paper_id:05d}\"\n",
    "        paper_ids.append(arxiv_id)\n",
    "    \n",
    "    # Thang sau: from 1 ton END_ID\n",
    "    for paper_id in range(1, END_ID + 1):\n",
    "        arxiv_id = f\"{END_YEAR_MONTH}.{paper_id:05d}\"\n",
    "        paper_ids.append(arxiv_id)\n",
    "    \n",
    "    logger.info(f\"Tong so papers: {len(paper_ids)}\")\n",
    "    logger.info(f\"Paper dau: {paper_ids[0]}\")\n",
    "    logger.info(f\"Paper cuoi: {paper_ids[-1]}\")\n",
    "    \n",
    "    # Sefromp try muc output\n",
    "    output_dir = DATA_DIR\n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    # Create scraper\n",
    "    scraper = ParallelArxivScraper(output_dir)\n",
    "    \n",
    "    # Check xem da co papers nao hoan thanh chua (to resume)\n",
    "    completed = set()\n",
    "    if os.path.exists(output_dir):\n",
    "        for item in os.listdir(output_dir):\n",
    "            item_path = os.path.join(output_dir, item)\n",
    "            if os.path.isdir(item_path) and '-' in item:\n",
    "                # Check xem paper nay da hoan thanh chua\n",
    "                metadata_file = os.path.join(item_path, \"metadata.json\")\n",
    "                references_file = os.path.join(item_path, \"references.json\")\n",
    "                if os.path.exists(metadata_file) and os.path.exists(references_file):\n",
    "                    arxiv_id = item.replace('-', '.')\n",
    "                    completed.add(arxiv_id)\n",
    "    \n",
    "    if completed:\n",
    "        logger.info(f\"Da co {len(completed)} papers hoan thanh, bo qua chung\")\n",
    "        paper_ids = [pid for pid in paper_ids if pid not in completed]\n",
    "        logger.info(f\"Con lai: {len(paper_ids)} papers\")\n",
    "    \n",
    "    # BAT DAU CAO!\n",
    "    logger.info(f\"\\nBAT DAU scrape with {MAX_WORKERS} workers!\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = scraper.scrape_papers_batch(paper_ids, batch_size=50)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # In ket qua\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"HOAN THANH!\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Thoi gian: {elapsed:.2f}s ({elapsed/60:.2f} phut)\")\n",
    "    logger.info(f\"Thanh cong: {results['successful']}\")\n",
    "    logger.info(f\"That bai: {results['failed']}\")\n",
    "    logger.info(f\"Tong: {results['total']}\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Bat dau do wall time\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    print(\"Dang run scraper...\")\n",
    "    print(\"\\nAndrious step (theo Lab 1):\")\n",
    "    print(\"  1. Entry Discovery - tim papers tren arXiv\")\n",
    "    print(\"  2. Download - tai source .tar.gz\")\n",
    "    print(\"  3. Remove hinh - chi giu .tex and .bib\")\n",
    "    print(\"  4. References - scrape data from Semantic Scholar\")\n",
    "    print(\"  5. Luu data - metadata.json, references.json\")\n",
    "    print(\"\\nRun parallel 6 workers!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Chuyen ando try muc src\n",
    "    os.chdir('/content/ScrapingData/23127240/src')\n",
    "    \n",
    "    # Run scraper with realtime output\n",
    "    process = subprocess.Popen(\n",
    "        ['python3', '-u', 'run_parallel.py'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        sttorr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Stream output realtime\n",
    "    return_code = None\n",
    "    while True:\n",
    "        line = process.stdout.readline()\n",
    "        if not line:\n",
    "            return_code = process.poll()\n",
    "            if return_code is not None:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "        \n",
    "        # In andrious dong quan in\n",
    "        if \"Progress:\" in line or \"Batch\" in line or \"HOAN THANH\" in line:\n",
    "            print(\"\\n\" + line.strip())\n",
    "        elif \"Scraping\" in line or \"Extracted\" in line:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(line, end=\"\", flush=True)\n",
    "    \n",
    "    # Doi process ket tryc\n",
    "    process.wait()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    if return_code != 0:\n",
    "        print(f\"Loi code: {return_code}\")\n",
    "    else:\n",
    "        print(\"Scraper xong!\")\n",
    "    \n",
    "    # Update metrics\n",
    "    monitor.update_metrics()\n",
    "    \n",
    "    # Ve lai try muc goc\n",
    "    os.chdir('/content/ScrapingData/23127240')\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nDung boi user\")\n",
    "    if 'process' in locals():\n",
    "        process.terminate()\n",
    "    os.chdir('/content/ScrapingData/23127240')\n",
    "except Exception as e:\n",
    "    print(f\"\\nLoi: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    os.chdir('/content/ScrapingData/23127240')\n",
    "finally:\n",
    "    # Ket tryc do wall time\n",
    "    metrics = monitor.finish(output_dir=\"23127240_data\")\n",
    "    \n",
    "    # Luu metrics\n",
    "    with open('performance_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, intont=2)\n",
    "    \n",
    "    print(\"\\nDa luu metrics: performance_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559d2ce",
   "metadata": {},
   "source": [
    "Step 6: Run scraper\n",
    "\n",
    "Sau khi run xong, andrious file se duoc create from dong:\n",
    "- paper_totails.csv - Chi tiet fromng paper  \n",
    "- scraping_stats.csv - Tong quan metrics\n",
    "- scraping_stats.json - Data day du\n",
    "\n",
    "Luu y:\n",
    "- KHONG tat Colab in khi scraper dang run\n",
    "- If bi ngat, run lai from dau - code from dong skip papers da scrape\n",
    "- Progress duoc luu moi 50 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf0e3e",
   "metadata": {},
   "source": [
    "Step 8: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Nen data\n",
    "print(\"Dang nen data...\")\n",
    "shutil.make_archive('23127240_data', 'zip', '.', '23127240_data')\n",
    "print(f\"Da create 23127240_data.zip\")\n",
    "\n",
    "# Check kich tryoc\n",
    "size_mb = os.path.getsize('23127240_data.zip') / (1024**2)\n",
    "print(f\"Kich tryoc: {size_mb:.2f} MB\")\n",
    "\n",
    "if size_mb > 100:\n",
    "    print(\"File lon hon 100MB, khuyen nghi upload len Google Drive\")\n",
    "    print(\"Run cell tiep theo to upload len Drive\")\n",
    "else:\n",
    "    print(\"\\nBat dau download...\")\n",
    "    files.download('23127240_data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ed4a1",
   "metadata": {},
   "source": [
    "Step 9: Upload len Google Drive (if file qua lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0315a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy ando Drive\n",
    "!cp 23127240_data.zip /content/drive/MyDrive/\n",
    "!cp performance_metrics.json /content/drive/MyDrive/\n",
    "\n",
    "print(\"Da upload ando Google Drive:\")\n",
    "print(\"   - 23127240_data.zip\")\n",
    "print(\"   - performance_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3903d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "GHI CHU\n",
    "\n",
    "Yeu cau Lab 1 da hoan thanh:\n",
    "- Testbed: Google Colab CPU-only mode\n",
    "- Wall time measurement (end-to-end)\n",
    "- Memory footprint (max RAM, disk usage)\n",
    "- Scrape: TeX sources, metadata, references\n",
    "- Remove figures to giam kich tryoc\n",
    "- Cau truc theo format yeu cau\n",
    "\n",
    "Rate Limiting:\n",
    "- Semantic Scholar: 1 req/s, 100 req/5min\n",
    "- Script co built-in retry mechanism\n",
    "\n",
    "Tomo Vitoo (<=120s):\n",
    "1. Sefromp (15s): Mo Colab, check CPU-only, clone repo\n",
    "2. Running (45s): Run scraper, show logs\n",
    "3. Results (45s): Performance metrics, verify strucfromre\n",
    "4. Withce: Giai thench scraper tosign and reasoning\n",
    "\n",
    "Lien he:\n",
    "- Instructor: hlhdang@fit.hcmus.edu.vn\n",
    "\n",
    "---\n",
    "\n",
    "Chuc ban scraping thanh cong!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}