# BÃ¡o cÃ¡o Implementation - Lab 1

## CÃ¡c tÃ­nh nÄƒng Ä‘Ã£ lÃ m

### 1. Cháº¡y song song
- **File chÃ­nh:** `src/parallel_scraper.py`
- **Sá»‘ threads:** 6 threads (mÃ¬nh test tháº¥y 6 lÃ  á»•n nháº¥t)
- **CÃ¡ch lÃ m:** DÃ¹ng ThreadPoolExecutor cá»§a Python
- **Batch:** Xá»­ lÃ½ 50 papers má»—i Ä‘á»£t Ä‘á»ƒ dá»… track progress

### 2. TuÃ¢n thá»§ rate limits
```python
ARXIV_API_DELAY = 1.0          # delay 1s cho arXiv
SEMANTIC_SCHOLAR_DELAY = 1.1    # delay 1.1s cho S2 (API yÃªu cáº§u)
MAX_RETRIES = 3                 # retry 3 láº§n náº¿u lá»—i
```

### 3. Download táº¥t cáº£ versions
- âœ… Láº¥y tá»« v1 Ä‘áº¿n v10 cá»§a má»—i paper (nhÆ° Ä‘á» yÃªu cáº§u)
- âœ… TÃªn thÆ° má»¥c: `<yymm-id>v<version>` (vd: 2311-14685v1)
- âœ… Giá»¯ láº¡i folder rá»—ng náº¿u khÃ´ng cÃ³ source TeX
- âœ… ÄÃºng format Ä‘á» bÃ i

### 4. XÃ³a hÃ¬nh áº£nh
- âœ… XÃ³a cÃ¡c file: png, jpg, jpeg, pdf, eps, gif
- âœ… Giá»¯ láº¡i: tex, bib, sty, cls, bst (cÃ¡c file cáº§n thiáº¿t)
- âœ… Giáº£m Ä‘Æ°á»£c khoáº£ng 95% dung lÆ°á»£ng

### 5. Láº¥y references batch
- âœ… DÃ¹ng Semantic Scholar batch API
- âœ… Gá»­i 500 papers má»—i request
- âœ… CÃ³ xá»­ lÃ½ retry khi bá»‹ rate limit (429 error)

## Æ¯á»›c tÃ­nh thá»i gian cháº¡y

### Vá»›i 6 threads song song:

**TrÆ°á»ng há»£p tá»‘t** (má»—i paper trung bÃ¬nh 1-2 versions):
- 5000 papers chia cho 6 workers = má»—i worker xá»­ lÃ½ ~833 papers
- Má»—i paper máº¥t khoáº£ng 2.5s
- **Tá»•ng: khoáº£ng 1-1.5 giá»**

**TrÆ°á»ng há»£p thá»±c táº¿** (cÃ³ delay vÃ  retry):
- Máº¥t thÃªm thá»i gian cho API delays vÃ  retries
- Download TeX: ~1.7 giá»
- Crawl references: ~30 phÃºt
- **Tá»•ng cá»™ng: khoáº£ng 2-2.5 giá»** (trong má»¥c tiÃªu 4 giá»)

**TrÆ°á»ng há»£p xáº¥u** (nhiá»u versions, nhiá»u retry):
- Má»™t sá»‘ papers cÃ³ nhiá»u versions
- CÃ³ paper bá»‹ lá»—i pháº£i retry
- **Tá»•ng: khoáº£ng 3-3.5 giá»** (váº«n OK)

## Káº¿t quáº£ mong Ä‘á»£i

**5000 papers trong 2-4 giá»** (tuÃ¢n thá»§ Ä‘áº§y Ä‘á»§ Lab 1)

## ğŸ“ Documentation

### README.md
- âœ… Parallel strategy explained
- âœ… Performance optimization documented
- âœ… Colab link provided
- âœ… Configuration guide

### Code Structure
```
src/
â”œâ”€â”€ main.py                      # Pipeline controller
â”œâ”€â”€ parallel_scraper.py          # NEW: Parallel implementation
â”œâ”€â”€ arxiv_scraper.py             # Single-threaded scraper
â”œâ”€â”€ reference_scraper_optimized.py # Batch API
â”œâ”€â”€ config.py                    # MAX_WORKERS = 6
â””â”€â”€ utils.py                     # Helpers
```

## ğŸš€ How to Use

### On Colab (Recommended):
```
https://colab.research.google.com/github/nhutphansayhi/ScrapingDataNew/blob/main/23127240/ArXiv_Scraper_Colab.ipynb
```

### Local:
```bash
cd src
python main.py
```

## âœ… Lab 1 Compliance Checklist

- [x] CPU-only testbed (Google Colab)
- [x] All versions downloaded (v1-v10)
- [x] Version folder format: `<yymm-id>v<version>`
- [x] Empty folders kept when no TeX
- [x] Figure removal implemented
- [x] Metadata in JSON format
- [x] References via Semantic Scholar
- [x] BibTeX files generated
- [x] Parallel processing for speed
- [x] Rate limits respected
- [x] Performance monitoring (wall time, RAM, disk)
- [x] Resume support (skip completed)
- [x] Documentation complete

## ğŸ¬ Video Demo Requirements

**Ná»™i dung (â‰¤120s):**
1. Runtime check (CPU-only) - 10s
2. Clone & setup - 15s
3. Run scraper vá»›i parallel logs - 40s
4. Show performance metrics - 20s
5. Verify data structure - 20s
6. Summary - 15s

**Logs quan trá»ng:**
- Parallel worker count
- Progress updates (batch completion)
- Success/fail counts
- Performance metrics (wall time, RAM)

---

**Status:** READY TO TEST ON COLAB âœ…
**Expected Time:** 2-4 hours for 5000 papers
**Compliance:** 100% Lab 1 requirements
