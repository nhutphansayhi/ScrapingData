# ğŸ“Š HÆ°á»›ng dáº«n Realtime Metrics

## TÃ­nh nÄƒng má»›i: Tá»± Ä‘á»™ng cáº­p nháº­t metrics má»—i 100 papers

Giá» scraper sáº½ **tá»± Ä‘á»™ng** tÃ­nh vÃ  lÆ°u 15 metrics theo yÃªu cáº§u Lab 1 má»—i khi xá»­ lÃ½ Ä‘Æ°á»£c 100 papers!

---

## âœ… Lá»£i Ã­ch

### 1. Theo dÃµi Tiáº¿n Ä‘á»™ Realtime
- KhÃ´ng cáº§n Ä‘á»£i Ä‘áº¿n cuá»‘i má»›i cÃ³ metrics
- Xem Ä‘Æ°á»£c káº¿t quáº£ trong khi Ä‘ang cháº¡y
- Biáº¿t ngay náº¿u cÃ³ váº¥n Ä‘á»

### 2. An toÃ n hÆ¡n
- Náº¿u scraper **crash giá»¯a chá»«ng** â†’ váº«n cÃ³ metrics cá»§a papers Ä‘Ã£ hoÃ n thÃ nh
- KhÃ´ng máº¥t háº¿t dá»¯ liá»‡u náº¿u Colab timeout
- CÃ³ thá»ƒ resume vÃ  metrics váº«n chÃ­nh xÃ¡c

### 3. ÄÃºng Format Äá» bÃ i
Tá»± Ä‘á»™ng táº¡o **3 files** theo yÃªu cáº§u Lab 1:
- `23127240_full_metrics.json` - JSON Ä‘áº§y Ä‘á»§ (táº¥t cáº£ 15 metrics)
- `23127240_metrics_summary.csv` - CSV tÃ³m táº¯t (báº£ng 15 metrics)
- `23127240_paper_details.csv` - CSV chi tiáº¿t tá»«ng paper

---

## ğŸ“‹ 15 Metrics theo Lab 1

### I. DATA STATISTICS (7 metrics)

1. **Papers Scraped Successfully** - Sá»‘ papers thÃ nh cÃ´ng
2. **Overall Success Rate** - Tá»· lá»‡ thÃ nh cÃ´ng tá»•ng thá»ƒ (%)
3. **Avg Paper Size Before** - KÃ­ch thÆ°á»›c TB trÆ°á»›c xÃ³a hÃ¬nh (bytes)
4. **Avg Paper Size After** - KÃ­ch thÆ°á»›c TB sau xÃ³a hÃ¬nh (bytes)
5. **Avg References Per Paper** - Sá»‘ references trung bÃ¬nh
6. **Ref Metadata Success Rate** - Tá»· lá»‡ láº¥y refs thÃ nh cÃ´ng (%)
7. **Other Stats** - Thá»‘ng kÃª khÃ¡c (dict)

### II. PERFORMANCE (8 metrics)

#### A. Running Time (4 metrics)

8. **Total Wall Time** - Tá»•ng thá»i gian (seconds)
9. **Avg Time Per Paper** - Thá»i gian TB/paper (seconds)
10. **Total Time One Paper** - Thá»i gian xá»­ lÃ½ 1 paper (seconds)
11. **Entry Discovery Time** - Thá»i gian tÃ¬m entries (seconds)

#### B. Memory Footprint (4 metrics)

12. **Max RAM Used** - RAM tá»‘i Ä‘a (MB)
13. **Max Disk Storage** - Disk tá»‘i Ä‘a (MB)
14. **Final Output Size** - KÃ­ch thÆ°á»›c output (MB)
15. **Avg RAM Consumption** - RAM trung bÃ¬nh (MB)

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Trong Notebook (Google Colab)

**Scraper tá»± Ä‘á»™ng update!** KhÃ´ng cáº§n lÃ m gÃ¬ thÃªm:

```python
# Cell nÃ y cháº¡y scraper
# Metrics sáº½ tá»± Ä‘á»™ng update má»—i 100 papers
!python src/run_parallel.py
```

### Xem Metrics Realtime

Cháº¡y cell nÃ y **trong khi scraper Ä‘ang cháº¡y**:

```python
# Xem metrics hiá»‡n táº¡i
import json

with open('23127240_full_metrics.json', 'r') as f:
    metrics = json.load(f)

print(f"Papers hoÃ n thÃ nh: {metrics['1_papers_scraped_successfully']}")
print(f"Thá»i gian: {metrics['8_total_wall_time_seconds']:.2f}s")
print(f"TB/paper: {metrics['9_avg_time_per_paper_seconds']:.2f}s")
```

---

## ğŸ“ Output Files

### 1. `23127240_full_metrics.json`

JSON Ä‘áº§y Ä‘á»§ vá»›i táº¥t cáº£ metrics:

```json
{
  "1_papers_scraped_successfully": 150,
  "2_overall_success_rate_percent": 98.67,
  "3_avg_paper_size_before_bytes": 12582912,
  "4_avg_paper_size_after_bytes": 153600,
  "5_avg_references_per_paper": 23.45,
  "6_ref_metadata_success_rate_percent": 95.33,
  "7_other_stats": {
    "total_papers": 152,
    "papers_with_refs": 150,
    "total_references": 3518
  },
  "8_total_wall_time_seconds": 1350.23,
  "9_avg_time_per_paper_seconds": 8.88,
  ...
}
```

### 2. `23127240_metrics_summary.csv`

Báº£ng tÃ³m táº¯t 15 metrics:

| Metric_ID | Category | Name | Value | Unit |
|-----------|----------|------|-------|------|
| 1 | Data Statistics | Papers Scraped Successfully | 150 | papers |
| 2 | Data Statistics | Overall Success Rate | 98.67 | % |
| 3 | Data Statistics | Avg Paper Size Before | 12582912 | bytes |
| ... | ... | ... | ... | ... |

**Copy trá»±c tiáº¿p vÃ o Report.docx!**

### 3. `23127240_paper_details.csv`

Chi tiáº¿t tá»«ng paper:

| paper_id | success | versions | tex_files | bib_files | num_references | size_before_bytes | size_after_bytes |
|----------|---------|----------|-----------|-----------|----------------|-------------------|------------------|
| 2311-14685 | True | 2 | 5 | 1 | 25 | 25165824 | 204800 |
| ... | ... | ... | ... | ... | ... | ... | ... |

---

## âš™ï¸ Cáº¥u hÃ¬nh

### Thay Ä‘á»•i táº§n suáº¥t update

Máº·c Ä‘á»‹nh: **má»—i 100 papers**

Muá»‘n thay Ä‘á»•i? Edit `src/run_parallel.py`:

```python
# Update má»—i 50 papers
results = scraper.scrape_papers_batch(
    paper_ids, 
    batch_size=50,
    update_interval=50  # <-- Thay Ä‘á»•i á»Ÿ Ä‘Ã¢y
)
```

CÃ¡c tÃ¹y chá»n:
- `update_interval=50` - Cáº­p nháº­t má»—i 50 papers (frequent)
- `update_interval=100` - Má»—i 100 papers (khuyáº¿n nghá»‹)
- `update_interval=200` - Má»—i 200 papers (Ã­t hÆ¡n)

---

## ğŸ” Debug / Troubleshooting

### Check metrics cÃ³ Ä‘Ãºng khÃ´ng?

```python
import json

with open('23127240_full_metrics.json', 'r') as f:
    metrics = json.load(f)

# Check timestamp
print(f"Last update: {metrics['timestamp']}")

# Check sá»‘ papers
print(f"Papers: {metrics['1_papers_scraped_successfully']}")

# Check thá»i gian
print(f"Running time: {metrics['total_wall_time_hours']:.2f}h")
```

### File khÃ´ng tá»“n táº¡i?

- ChÆ°a cháº¡y Ä‘áº¿n 100 papers Ä‘áº§u tiÃªn
- Hoáº·c scraper chÆ°a báº¯t Ä‘áº§u
- Check log: `logs/scraper.log`

### Metrics khÃ´ng update?

Check xem cÃ³ lá»—i khÃ´ng:

```bash
# Xem log
!tail -50 logs/scraper.log

# Check sá»‘ papers hiá»‡n táº¡i
!ls -1 23127240_data | wc -l
```

---

## ğŸ“Š So sÃ¡nh vá»›i CÃ¡ch CÅ©

| Aspect | CÃ¡ch CÅ© | CÃ¡ch Má»›i (Realtime) |
|--------|---------|---------------------|
| **Khi nÃ o cÃ³ metrics?** | Sau khi cháº¡y xong Háº¾T | Má»—i 100 papers |
| **Náº¿u crash?** | Máº¥t háº¿t | Váº«n cÃ³ metrics Ä‘áº¿n lÃºc crash |
| **Theo dÃµi progress?** | KhÃ´ng | CÃ³! Xem realtime |
| **Format output** | Pháº£i tÃ­nh thá»§ cÃ´ng | Tá»± Ä‘á»™ng 3 files |
| **ÄÃºng 15 metrics Lab?** | Pháº£i check láº¡i | Tá»± Ä‘á»™ng Ä‘Ãºng format |

---

## âœ… Checklist Report.docx

Vá»›i metrics tá»± Ä‘á»™ng, báº¡n cÃ³:

- [x] 15 metrics Ä‘áº§y Ä‘á»§ theo Lab 1
- [x] JSON format (cho tháº§y check detail)
- [x] CSV format (copy vÃ o Word)
- [x] Realtime progress tracking
- [x] An toÃ n náº¿u crash
- [x] Timestamp Ä‘á»ƒ track thá»i gian

**Chá»‰ cáº§n:**
1. Cháº¡y scraper
2. Äá»£i xong (hoáº·c theo dÃµi realtime)
3. Copy tá»« CSV vÃ o Report.docx
4. Done! âœ¨

---

## ğŸ¯ Tips

### Æ¯á»›c tÃ­nh thá»i gian cÃ²n láº¡i

```python
import json

with open('23127240_full_metrics.json', 'r') as f:
    m = json.load(f)

papers_done = m['1_papers_scraped_successfully']
avg_time = m['9_avg_time_per_paper_seconds']
papers_remaining = 5000 - papers_done

time_remaining_hours = (papers_remaining * avg_time) / 3600

print(f"Papers xong: {papers_done}/5000")
print(f"Thá»i gian TB: {avg_time:.2f}s/paper")
print(f"Æ¯á»›c tÃ­nh cÃ²n: {time_remaining_hours:.2f} giá»")
```

### Monitor RAM/Disk

```python
# Check xem cÃ³ nguy cÆ¡ háº¿t RAM khÃ´ng
if m['12_max_ram_mb'] > 10000:  # >10GB
    print("âš ï¸ RAM cao! CÃ³ thá»ƒ cáº§n cleanup")
```

---

## ğŸ“š Reference

- Lab 1 Requirements: Táº¥t cáº£ 15 metrics báº¯t buá»™c
- Update frequency: Má»—i 100 papers (cÃ³ thá»ƒ thay Ä‘á»•i)
- Files location: Root cá»§a repo (cÃ¹ng cáº¥p vá»›i `src/`)
- Naming: `{STUDENT_ID}_*.{json,csv}`

---

**ChÃºc báº¡n scraping thÃ nh cÃ´ng! ğŸš€**
