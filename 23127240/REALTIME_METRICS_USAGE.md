# ğŸš€ Realtime Metrics Update - HÆ°á»›ng dáº«n sá»­ dá»¥ng

## TÃ­nh nÄƒng má»›i

Script Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ **tá»± Ä‘á»™ng táº¡o vÃ  cáº­p nháº­t file thá»‘ng kÃª má»—i 50 papers**!

## ğŸ“Š Files Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng

### 1. `paper_details.csv`
**Cáº­p nháº­t:** Má»—i 50 papers  
**Ná»™i dung:** Chi tiáº¿t tá»«ng paper Ä‘Ã£ scrape

Format:
```csv
paper_id,arxiv_id,title,authors,runtime_s,size_before,size_after,size_before_figures,size_after_figures,num_refs,current_output_size,max_rss,avg_rss,processed_at
1,2311.14859,Paper Title,Authors,10.16,18979,18979,18979,18979,28,20789632,623.17,27.48,2025-11-14 09:41:14
```

### 2. `scraping_stats.csv`
**Cáº­p nháº­t:** Má»—i 50 papers  
**Ná»™i dung:** Tá»•ng quan 15 metrics theo Lab 1

Sections:
- General Info (Student ID, range)
- Data Statistics (7 metrics)
- Performance - Running Time (4 metrics)
- Performance - Memory Footprint (4 metrics)

### 3. `scraping_stats.json`
**Cáº­p nháº­t:** Má»—i 50 papers  
**Ná»™i dung:** Dá»¯ liá»‡u Ä‘áº§y Ä‘á»§ dáº¡ng JSON

Structure:
```json
{
  "general_info": {...},
  "data_statistics": {...},
  "performance_running_time": {...},
  "performance_memory_footprint": {...}
}
```

## ğŸ¯ Sá»­ dá»¥ng trÃªn Google Colab

### BÆ°á»›c 1: Cháº¡y scraper
```python
# Cell "Cháº¡y Scraper"
# Scraper sáº½ tá»± Ä‘á»™ng cáº­p nháº­t metrics má»—i 50 papers
```

### BÆ°á»›c 2: Xem metrics realtime
```python
# Cell "Xem Metrics Realtime"
# Cháº¡y cell nÃ y má»—i vÃ i phÃºt Ä‘á»ƒ xem tiáº¿n Ä‘á»™
import pandas as pd

df = pd.read_csv('23127240_data/paper_details.csv')
print(f"Papers processed: {len(df)}")
print(f"Last update: {df.iloc[-1]['processed_at']}")
```

### BÆ°á»›c 3: Download files (optional)
```python
# Cell "Download Files"
from google.colab import files
files.download('23127240_data/paper_details.csv')
files.download('23127240_data/scraping_stats.csv')
```

## ğŸ” Xem metrics tá»« terminal (local)

Náº¿u cháº¡y local, cÃ³ thá»ƒ dÃ¹ng script helper:

```bash
cd src
python view_metrics.py
```

Script nÃ y sáº½:
- Hiá»ƒn thá»‹ metrics realtime
- Tá»± Ä‘á»™ng refresh má»—i 30 giÃ¢y
- Show last 5 papers processed
- Show performance stats

## âš™ï¸ CÃ¡ch hoáº¡t Ä‘á»™ng

### Timeline cáº­p nháº­t:

```
Papers 1-9:   â±ï¸  Progress log only
Paper 10:     ğŸ“ Quick save (JSON only)
Papers 11-19: â±ï¸  Progress log only
Paper 20:     ğŸ“ Quick save (JSON only)
...
Paper 50:     ğŸ’¾ FULL CHECKPOINT
               âœ… paper_details.csv updated
               âœ… scraping_stats.csv updated
               âœ… scraping_stats.json updated
Papers 51-59: â±ï¸  Progress log only
Paper 60:     ğŸ“ Quick save (JSON only)
...
Paper 100:    ğŸ’¾ FULL CHECKPOINT
               (files updated again)
```

### Log messages báº¡n sáº½ tháº¥y:

```
ğŸ“Š PROGRESS UPDATE
==================================================================
Papers processed: 50
  âœ… Successful: 48
  âŒ Failed: 2
  ğŸ“ˆ Success rate: 96.0%
  ğŸ’¾ RAM: 1234.5 MB (max: 1500.0 MB)
  ğŸ’¿ Disk: 2345.6 MB
==================================================================

ğŸ’¾ CHECKPOINT: Saving full statistics at paper 50/5000
ğŸ“„ Paper details CSV updated: 23127240_data/paper_details.csv
   Total papers tracked: 48
âœ… CSV files updated: paper_details.csv & scraping_stats.csv
```

## ğŸ“ˆ VÃ­ dá»¥ sá»­ dá»¥ng CSV

### Python/Pandas
```python
import pandas as pd

# Load chi tiáº¿t papers
df = pd.read_csv('23127240_data/paper_details.csv')

# TÃ­nh cÃ¡c metrics
print(f"Total papers: {len(df)}")
print(f"Avg runtime: {df['runtime_s'].mean():.2f}s")
print(f"Avg references: {df['num_refs'].mean():.2f}")

# Filter papers cÃ³ nhiá»u references
high_ref_papers = df[df['num_refs'] > 30]
print(f"Papers with >30 refs: {len(high_ref_papers)}")

# Plot distribution
import matplotlib.pyplot as plt
df['runtime_s'].hist(bins=50)
plt.title('Runtime Distribution')
plt.xlabel('Runtime (seconds)')
plt.show()
```

### Excel/Google Sheets
1. Download `paper_details.csv`
2. Má»Ÿ báº±ng Excel/Sheets
3. Táº¡o pivot tables, charts
4. PhÃ¢n tÃ­ch data cho report

## ğŸ“ ÄÃ¡p á»©ng yÃªu cáº§u Lab 1

Files nÃ y chá»©a Ä‘áº§y Ä‘á»§ 15 metrics yÃªu cáº§u:

### I. Data Statistics (7 metrics)
âœ… 1. Papers scraped successfully  
âœ… 2. Overall success rate (%)  
âœ… 3. Avg paper size before (bytes)  
âœ… 4. Avg paper size after (bytes)  
âœ… 5. Avg references per paper  
âœ… 6. Reference metadata success rate (%)  
âœ… 7. Other statistics (total refs, versions, etc.)  

### II. Performance (8 metrics)

#### A. Running Time (4 metrics)
âœ… 8. Total wall time (seconds)  
âœ… 9. Avg time per paper (seconds)  
âœ… 10. Total time one paper (seconds)  
âœ… 11. Entry discovery time (seconds)  

#### B. Memory Footprint (4 metrics)
âœ… 12. Maximum RAM used (MB)  
âœ… 13. Maximum disk storage required (MB)  
âœ… 14. Final output storage size (MB)  
âœ… 15. Average RAM consumption (MB)  

## ğŸ’¡ Tips

1. **Theo dÃµi tiáº¿n Ä‘á»™:** Cháº¡y cell "Xem Metrics Realtime" má»—i 5-10 phÃºt
2. **Backup thÆ°á»ng xuyÃªn:** Download CSV sau má»—i 100-200 papers
3. **Monitor RAM:** Náº¿u RAM gáº§n Ä‘áº§y, restart Colab runtime
4. **Kiá»ƒm tra logs:** Scroll logs Ä‘á»ƒ xem papers nÃ o failed

## ğŸ› Troubleshooting

### File chÆ°a Ä‘Æ°á»£c táº¡o
**NguyÃªn nhÃ¢n:** ChÆ°a Ä‘á»§ 50 papers  
**Giáº£i phÃ¡p:** Äá»£i scraper xá»­ lÃ½ Ä‘á»§ 50 papers

### CSV khÃ´ng update
**NguyÃªn nhÃ¢n:** Scraper bá»‹ crash trÆ°á»›c checkpoint  
**Giáº£i phÃ¡p:** Cháº¡y láº¡i, progress Ä‘Æ°á»£c lÆ°u tá»« checkpoint trÆ°á»›c

### RAM overflow trÃªn Colab
**NguyÃªn nhÃ¢n:** QuÃ¡ nhiá»u papers trong memory  
**Giáº£i phÃ¡p:** 
- Runtime > Restart runtime
- Cháº¡y láº¡i, script sáº½ skip papers Ä‘Ã£ scrape
- Checkpoint Ä‘Ã£ Ä‘Æ°á»£c lÆ°u má»—i 50 papers

## ğŸ“ Support

Náº¿u cÃ³ váº¥n Ä‘á»:
1. Check logs trong Colab
2. Xem file `logs/scraper.log`
3. Kiá»ƒm tra file README_METRICS.md trong thÆ° má»¥c data

---

**Happy Scraping! ğŸ‰**
